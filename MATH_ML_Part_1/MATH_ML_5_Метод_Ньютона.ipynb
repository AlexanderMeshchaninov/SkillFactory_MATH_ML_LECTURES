{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод Ньютона - алгоритм оптимизации (работает быстрее чем градиентный спуск).\n",
    "\n",
    "Он основан на приближённом решении нелинейных уравнений с помощью касательных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример:\n",
    "\n",
    "Найдите третий корень полинома $f(x) = 6x^5 - 5x^4 - 4x^3 + 3x^2$, взяв за точку старта 0.7. Введите получившееся значение с точностью до трёх знаков после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration №: 1\n",
      "f(xn) = -0.09407999999999994\n",
      "f(xn) prime = -1.3369999999999997\n",
      "---> Сurrent ROOT <--- : 0.7\n",
      "________________________________________\n",
      "Iteration №: 2\n",
      "f(xn) = -0.0012133284487552132\n",
      "f(xn) prime = -1.2567755749164586\n",
      "---> Сurrent ROOT <--- : 0.63\n",
      "________________________________________\n",
      "Iteration №: 3\n",
      "f(xn) = -1.3785387997788945e-06\n",
      "f(xn) prime = -1.2539130914128895\n",
      "---> Сurrent ROOT <--- : 0.629\n",
      "________________________________________\n",
      "Iteration №: 4\n",
      "f(xn) = -1.8043344596208044e-12\n",
      "f(xn) prime = -1.2539098089231775\n",
      "---> Сurrent ROOT <--- : 0.629\n",
      "________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.629"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Функция f(x), корни которой мы ищем\n",
    "def f(x):\n",
    "    return 6 * x**5 - 5 * x**4 - 4 * x**3 + 3 * x**2\n",
    "\n",
    "# Производная функции f(x) — градиент (первая производная)\n",
    "def grad(x):\n",
    "    dx = 30 * x**4 - 20 * x**3 - 12 * x**2 + 6 * x\n",
    "    return dx\n",
    "\n",
    "# Реализация метода Ньютона для поиска корней уравнения f(x) = 0\n",
    "def newtons_algo_root(f, grad, x0=0, tol=0.0001, count_val=3):\n",
    "    \"\"\"\n",
    "    Метод Ньютона для поиска корня уравнения f(x) = 0.\n",
    "    \n",
    "    Параметры:\n",
    "    f - функция, корень которой ищем\n",
    "    grad - производная функции f\n",
    "    x0 - начальное приближение\n",
    "    iter_count - максимальное количество итераций\n",
    "    \"\"\"\n",
    "\n",
    "    # Текущая точка (начальное приближение)\n",
    "    x_cur = x0\n",
    "    \n",
    "    # Предыдущая точка (нужна для остановки алгоритма)\n",
    "    x_pred = None\n",
    "    \n",
    "    ite = 1\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        fxn = f(x_cur)  # Вычисляем значение функции f(x)\n",
    "        f_prime = grad(x_cur)  # Вычисляем значение производной f'(x)\n",
    "        \n",
    "        # Проверка деления на 0 (если производная равна 0, метод не применим)\n",
    "        if f_prime == 0:\n",
    "            print(\"Производная равна 0. Метод Ньютона не применим.\")\n",
    "            return None\n",
    "        \n",
    "        # Формула метода Ньютона: x_(n+1) = x_n - f(x_n) / f'(x_n)\n",
    "        x_new = x_cur - fxn / f_prime\n",
    "        \n",
    "        # Вывод промежуточных результатов\n",
    "        print(f'Iteration №: {ite}')\n",
    "        print(f'f(xn) = {fxn}')  # Текущее значение функции\n",
    "        print(f'f(xn) prime = {f_prime}')  # Значение производной\n",
    "        print(f'---> Сurrent ROOT <--- : {round(x_cur, count_val)}')  # Текущий корень\n",
    "        print('__' * 20)\n",
    "    \n",
    "        # Проверка критерия остановки (если разница между x_new и x_pred мала)\n",
    "        if x_pred is not None and abs(x_pred - x_new) < tol:  break\n",
    "        \n",
    "        # Обновляем x_pred и x_cur для следующей итерации\n",
    "        x_pred = x_cur\n",
    "        x_cur = x_new\n",
    "        \n",
    "        ite +=1\n",
    "    return round(x_cur, count_val)\n",
    "\n",
    "# Вызов метода Ньютона с начальным приближением x0 = 0.7 и 5 итерациями\n",
    "newtons_algo_root(\n",
    "    f=f,\n",
    "    grad=grad,\n",
    "    x0=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример:\n",
    "\n",
    "Оптимизировать функцию (найти точку минимума): $f(x) = x^3 - 3x^2 - 45x + 40$. Начальной точки нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration №: 1\n",
      "f(xn) = 66946\n",
      "f(xn) prime = 4995\n",
      "f(xxn) second = 246\n",
      "---> Сurrent MINIMUM <--- : 42\n",
      "________________________________________\n",
      "Iteration №: 2\n",
      "f(xn) = 7863.108038551384\n",
      "f(xn) prime = 1236.864217727543\n",
      "f(xxn) second = 124.17073170731709\n",
      "---> Сurrent MINIMUM <--- : 21.695\n",
      "________________________________________\n",
      "Iteration №: 3\n",
      "f(xn) = 714.5634836428904\n",
      "f(xn) prime = 297.6643508293206\n",
      "f(xxn) second = 64.40475300745938\n",
      "---> Сurrent MINIMUM <--- : 11.734\n",
      "________________________________________\n",
      "Iteration №: 4\n",
      "f(xn) = -72.03041746416682\n",
      "f(xn) prime = 64.0824440979098\n",
      "f(xxn) second = 36.67409616029981\n",
      "---> Сurrent MINIMUM <--- : 7.112\n",
      "________________________________________\n",
      "Iteration №: 5\n",
      "f(xn) = -133.35266928891218\n",
      "f(xn) prime = 9.15968525359429\n",
      "f(xxn) second = 26.190002349047845\n",
      "---> Сurrent MINIMUM <--- : 5.365\n",
      "________________________________________\n",
      "Iteration №: 6\n",
      "f(xn) = -134.99720180515925\n",
      "f(xn) prime = 0.3669537086002208\n",
      "f(xxn) second = 24.091563762097362\n",
      "---> Сurrent MINIMUM <--- : 5.015\n",
      "________________________________________\n",
      "Iteration №: 7\n",
      "f(xn) = -134.99999998990785\n",
      "f(xn) prime = 0.0006960073662583\n",
      "f(xxn) second = 24.000174001210805\n",
      "---> Сurrent MINIMUM <--- : 5.0\n",
      "________________________________________\n",
      "Iteration №: 8\n",
      "f(xn) = -135.0\n",
      "f(xn) prime = 2.5230164624190365e-09\n",
      "f(xxn) second = 24.000000000630756\n",
      "---> Сurrent MINIMUM <--- : 5.0\n",
      "________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определяем функцию f(x), минимум которой мы ищем\n",
    "def f(x):\n",
    "    return x**3 - 3 * x**2 - 45 * x + 40\n",
    "\n",
    "# Производные функции f(x)\n",
    "def grad(x):\n",
    "    dx = 3*x**2 - 6*x - 45  # Первая производная f'(x) (градиент)\n",
    "    dxx = 6 * x - 6         # Вторая производная f''(x) (для проверки минимума)\n",
    "    return [dx, dxx]        # Возвращаем список значений первой и второй производных\n",
    "\n",
    "# Метод Ньютона для поиска минимума функции\n",
    "def newtons_algo_minimum(f, grad, x0=0, tol=0.0001, count_val=3):\n",
    "    \"\"\"\n",
    "    Метод Ньютона для поиска минимума функции f(x).\n",
    "    \n",
    "    Параметры:\n",
    "    f - функция, минимум которой ищем\n",
    "    grad - функция, возвращающая первую и вторую производные\n",
    "    x0 - начальная точка\n",
    "    \"\"\"\n",
    "\n",
    "    # Текущая точка (начальное приближение)\n",
    "    x_cur = x0\n",
    "    \n",
    "    # Предыдущая точка (для критерия остановки)\n",
    "    x_pred = None\n",
    "    \n",
    "    # Счетчик итераций\n",
    "    ite = 1\n",
    "\n",
    "    # Основной цикл метода Ньютона\n",
    "    while True:\n",
    "        fxn = f(x_cur)            # Вычисляем значение функции в текущей точке\n",
    "        f_prime = grad(x_cur)[0]  # Первая производная (градиент)\n",
    "        f_second = grad(x_cur)[1] # Вторая производная (используется в формуле)\n",
    "\n",
    "        # Проверка деления на 0 (если производные равны 0, метод не применим)\n",
    "        if (f_prime == 0) or (f_second == 0):\n",
    "            print(\"Производная равна 0. Метод Ньютона не применим.\")\n",
    "            return None\n",
    "\n",
    "        # Формула метода Ньютона для поиска минимума:\n",
    "        # x_(n+1) = x_n - f'(x_n) / f''(x_n)\n",
    "        x_new = x_cur - f_prime / f_second\n",
    "\n",
    "        # Вывод промежуточных результатов\n",
    "        print(f'Iteration №: {ite}')\n",
    "        print(f'f(xn) = {fxn}')            # Текущее значение функции\n",
    "        print(f'f(xn) prime = {f_prime}')  # Первая производная f'(x)\n",
    "        print(f'f(xxn) second = {f_second}')  # Вторая производная f''(x)\n",
    "        print(f'---> Сurrent MINIMUM <--- : {round(x_cur, count_val)}')\n",
    "        print('__' * 20)\n",
    "\n",
    "        # Критерий остановки: если разница между новой и предыдущей точкой < 0.0001\n",
    "        if x_pred is not None and abs(x_pred - x_new) < tol:\n",
    "            break\n",
    "\n",
    "        # Обновляем x_pred и x_cur для следующей итерации\n",
    "        x_pred = x_cur\n",
    "        x_cur = x_new\n",
    "\n",
    "        # Увеличиваем счетчик итераций\n",
    "        ite += 1\n",
    "    return round(x_cur, count_val)\n",
    "\n",
    "# Запускаем метод Ньютона с начальным приближением x0 = 42\n",
    "newtons_algo_minimum(\n",
    "    f=f,\n",
    "    grad=grad,\n",
    "    x0=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тоже самое из библиотеки scipy.optimizee: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import newton\n",
    "\n",
    "def func1(x):\n",
    "    return 3*x**2 - 6*x -45\n",
    "def func2(x):\n",
    "    return 6 * x - 6\n",
    "\n",
    "newton(func=func1, fprime=func2, x0=50, tol=0.0001)\n",
    "# 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Достоинства метода:\n",
    "\n",
    "- Если мы минимизируем квадратичную функцию, то с помощью метода Ньютона можно попасть в минимум целевой функции за один шаг.\n",
    "\n",
    "- Также этот алгоритм сходится за один шаг, если в качестве минимизируемой функции выступает функция из класса поверхностей вращения (т.е. такая, у которой есть симметрия).\n",
    "\n",
    "- Для несимметричной функции метод не может обеспечить сходимость, однако скорость сходимости  всё равно превышает скорость методов, основанных на градиентном спуске.\n",
    "\n",
    "# Недостатки метода:\n",
    "\n",
    "- Этот метод очень чувствителен к изначальным условиям.\n",
    "\n",
    "- При использовании градиентного спуска мы всегда гарантированно движемся по антиградиенту в сторону минимума. В методе Ньютона происходит подгонка параболоида к локальной кривизне, и затем алгоритм движется к неподвижной точке данного параболоида. Из-за этого мы можем попасть в максимум или седловую точку. Особенно ярко это видно на невыпуклых функциях с большим количеством переменных, так как у таких функций седловые точки встречаются намного чаще экстремумов.\n",
    "\n",
    "- Поэтому здесь необходимо обозначить ограничение: метод Ньютона стоит применять только для задач, в которых целевая функция выпуклая.\n",
    "\n",
    "- Впрочем, это не является проблемой. В линейной регрессии или при решении задачи классификации с помощью метода опорных векторов или логистической регрессии мы как раз ищем минимум у выпуклой целевой функции, то есть данный алгоритм подходит нам во многих случаях.\n",
    "\n",
    "- Также метод Ньютона может быть затратным с точки зрения вычислительной сложности, так как требует вычисления не только градиента, но и гессиана и обратного гессиана (при делении на матрицу необходимо искать обратную матрицу).\n",
    "\n",
    "- !!!Если у задачи много параметров, то расходы на память и время вычислений становятся астрономическими. Например, при наличии 50 параметров нужно вычислять более 1000 значений на каждом шаге, а затем предстоит ещё более 500 операций нахождения обратной матрицы. Однако метод всё равно используют, так как выгода от быстрой сходимости перевешивает затраты на вычисления.!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration №: 1\n",
      "f(xn) = 588726\n",
      "f(xn) prime = 42168\n",
      "f(xxn) second = 2012\n",
      "---> Сurrent MINIMUM <--- : 42\n",
      "________________________________________\n",
      "Iteration №: 2\n",
      "f(xn) = 73195.24536001583\n",
      "f(xn) prime = 10541.958333498018\n",
      "f(xxn) second = 1006.0039761431411\n",
      "---> Сurrent MINIMUM <--- : 21.042\n",
      "________________________________________\n",
      "Iteration №: 3\n",
      "f(xn) = 8754.77428010076\n",
      "f(xn) prime = 2635.44791736657\n",
      "f(xxn) second = 503.0099403264221\n",
      "---> Сurrent MINIMUM <--- : 10.563\n",
      "________________________________________\n",
      "Iteration №: 4\n",
      "f(xn) = 700.1520011704947\n",
      "f(xn) prime = 658.820315309824\n",
      "f(xxn) second = 251.5208744214744\n",
      "---> Сurrent MINIMUM <--- : 5.323\n",
      "________________________________________\n",
      "Iteration №: 5\n",
      "f(xn) = -306.4575186250329\n",
      "f(xn) prime = 164.66342269884942\n",
      "f(xxn) second = 125.7922437159364\n",
      "---> Сurrent MINIMUM <--- : 2.704\n",
      "________________________________________\n",
      "Iteration №: 6\n",
      "f(xn) = -432.1746519343719\n",
      "f(xn) prime = 41.124231138972554\n",
      "f(xxn) second = 62.95971878384913\n",
      "---> Сurrent MINIMUM <--- : 1.395\n",
      "________________________________________\n",
      "Iteration №: 7\n",
      "f(xn) = -447.83491662831267\n",
      "f(xn) prime = 10.239559301497358\n",
      "f(xxn) second = 31.60692476252232\n",
      "---> Сurrent MINIMUM <--- : 0.742\n",
      "________________________________________\n",
      "Iteration №: 8\n",
      "f(xn) = -449.7655609623062\n",
      "f(xn) prime = 2.5188904942494625\n",
      "f(xxn) second = 16.05657147238938\n",
      "---> Сurrent MINIMUM <--- : 0.418\n",
      "________________________________________\n",
      "Iteration №: 9\n",
      "f(xn) = -449.99402351695824\n",
      "f(xn) prime = 0.5906418055774958\n",
      "f(xxn) second = 8.526524106307306\n",
      "---> Сurrent MINIMUM <--- : 0.261\n",
      "________________________________________\n",
      "Iteration №: 10\n",
      "f(xn) = -450.0171398964744\n",
      "f(xn) prime = 0.11516368668835197\n",
      "f(xxn) second = 5.201510734592574\n",
      "---> Сurrent MINIMUM <--- : 0.192\n",
      "________________________________________\n",
      "Iteration №: 11\n",
      "f(xn) = -450.0185016089474\n",
      "f(xn) prime = 0.011764767859253622\n",
      "f(xxn) second = 4.13877007267719\n",
      "---> Сurrent MINIMUM <--- : 0.17\n",
      "________________________________________\n",
      "Iteration №: 12\n",
      "f(xn) = -450.01851851381946\n",
      "f(xn) prime = 0.0001939256991766447\n",
      "f(xxn) second = 4.002326431854474\n",
      "---> Сurrent MINIMUM <--- : 0.167\n",
      "________________________________________\n",
      "Iteration №: 13\n",
      "f(xn) = -450.01851851851853\n",
      "f(xn) prime = 5.6345204502150636e-08\n",
      "f(xxn) second = 4.000000676142395\n",
      "---> Сurrent MINIMUM <--- : 0.167\n",
      "________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.167"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определяем функцию f(x), минимум которой мы ищем\n",
    "def f(x):\n",
    "    return 8*x**3 - 2*x**2 - 450\n",
    "\n",
    "# Производные функции f(x)\n",
    "def grad(x):\n",
    "    dx = 24*x**2 - 4*x\n",
    "    dxx = 48*x - 4\n",
    "    return [dx, dxx]        # Возвращаем список значений первой и второй производных\n",
    "\n",
    "# Запускаем метод Ньютона с начальным приближением x0 = 42\n",
    "newtons_algo_minimum(\n",
    "    f=f,\n",
    "    grad=grad,\n",
    "    x0=42,\n",
    "    tol=0.0001,\n",
    "    count_val=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Квазиньютоновские методы (сочетание градиентного спуска и метода ньютона)\n",
    "\n",
    "В методе Ньютона мы обновляем точку на каждой итерации и вычисляется гессиан, а также его обратная матрица, что является затратным с точки зрения вычислений, но обеспечивает более быструю сходимость.\n",
    "\n",
    "В квазиньютоновских методах вместо вычисления гессиана мы просто аппроксимируем его матрицей, которая обновляется от итерации к итерации с использованием информации, вычисленной на предыдущих шагах. Так как вместо вычисления большого количества новых величин мы использует найденные ранее значения, квазиньютоновский алгоритм тратит гораздо меньше времени и вычислительных ресурсов.\n",
    "\n",
    "Эти способы объединены **ограничением**: процесс обновления матрицы должен быть достаточно эффективным и не должен требовать вычислений гессиана. То есть, по сути, на каждом шаге мы должны получать информацию о гессиане, не находя непосредственно сам гессиан.\n",
    "\n",
    "Три самые популярные схемы аппроксимации:\n",
    "\n",
    "- симметричная коррекция ранга 1 (SR1);\n",
    "\n",
    "- схема Дэвидона — Флетчера — Пауэлла (DFP);\n",
    "\n",
    "- схема Бройдена — Флетчера — Гольдфарба — Шанно (BFGS).\n",
    "\n",
    "Последняя схема (BFGS) самая известная, стабильная и считается наиболее эффективной. На ней мы и остановимся.\n",
    "\n",
    "У этой схемы есть две известных вариации:\n",
    "\n",
    "- L-BFGS;\n",
    "\n",
    "- L-BFGS-B.\n",
    "\n",
    "Обе этих вариации необходимы в случае большого количества переменных для экономии памяти (так как во время их реализации хранится ограниченное количество информации). По сути, они работают одинаково, и L-BFGS-B является лишь улучшенной версией L-BFGS для работы с ограничениями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим, квазиньютоновский метод к функции: $f(x, y) = x^2 + y^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию, которую будем оптимизировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации Optimization terminated successfully.\n",
      "Количество оценок: 9\n",
      "Решение: f([-1.07505143e-08 -1.07505143e-08]) = 0.00000\n"
     ]
    }
   ],
   "source": [
    "# Определяем целевую функцию\n",
    "def f(xy):\n",
    "    x, y = xy\n",
    "    return x**2 + y**2\n",
    " \n",
    "# определяем начальную точку\n",
    "x_0 = [1, 1]\n",
    "\n",
    "# реализуем алгоритм L-BFGS-B\n",
    "result = minimize(f, x_0, method='BFGS')\n",
    "\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = f(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы получили, что минимум функции достигается в точке (-1.07505143e-08 -1.07505143e-08), что практически = (0,0). Значение функции в этой точке также равно нулю.\n",
    "\n",
    "Можно повторить то же самое с вариацией  L-BFGS-B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "Количество оценок: 9\n",
      "Решение: f([-6.93562979e-09 -6.93562979e-09]) = 0.00000\n"
     ]
    }
   ],
   "source": [
    "# реализуем алгоритм L-BFGS-B\n",
    "result = minimize(f, x_0, method='L-BFGS-B')\n",
    "\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = f(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда количество итераций у двух модификаций различается, но ответ совпадает. Бывает также, что одна из вариаций может не сойтись, а другая — достичь экстремума, поэтому советуем не воспринимать их как взаимозаменяемые алгоритмы. На практике лучше пробовать разные варианты: если у вас не сошёлся алгоритм BFGS, можно попробовать L-BFGS-B, и наоборот."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации Optimization terminated successfully.\n",
      "Количество оценок: 111\n",
      "Решение: f([ 1.31669611e-02 -6.88585813e-09]) = 10.00000\n",
      "Статус оптимизации CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "Количество оценок: 120\n",
      "Решение: f([-9.59527042e-03 -2.14899286e-06]) = 10.00000\n"
     ]
    }
   ],
   "source": [
    "# определяем нашу функцию\n",
    "def func(x):\n",
    "    return x[0]**4 + 6 * x[1]**2 + 10\n",
    " \n",
    "# определяем начальную точку\n",
    "x_0 = np.array([100, 100])\n",
    "\n",
    "# реализуем алгоритм L-BFGS-B\n",
    "result = minimize(func, x_0, method='BFGS')\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))\n",
    "\n",
    "# реализуем алгоритм L-BFGS-B\n",
    "result = minimize(func, x_0, method='L-BFGS-B')\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
